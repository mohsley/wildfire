{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4035f97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import subprocess\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9527f8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "base_dir = \"/mnt/d/school/wildfire/hysplit/organized/hysplit/HYSPLIT\"\n",
    "working_dir = os.path.join(base_dir, \"working\")\n",
    "exec_dir = os.path.join(base_dir, \"exec\")\n",
    "bdy_files_dir = os.path.join(base_dir, \"bdyfiles\")\n",
    "graphics_dir = os.path.join(base_dir, \"graphics\")\n",
    "output_dir = \"/mnt/d/school/wildfire/hysplit/organized/data/conc_output\"\n",
    "\n",
    "# Meteorological data file\n",
    "met_file = \"gdas1.jan25.w2\"  # Adjust to your actual file\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "    print(f\"Created output directory: {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82812600",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_control_file(output_dir, met_dir, met_file):\n",
    "    \"\"\"Create a HYSPLIT CONTROL file for PM2.5 concentration simulation\"\"\"\n",
    "    control_path = os.path.join(met_dir, \"CONTROL\")\n",
    "    \n",
    "    # Make sure output directory exists\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    with open(control_path, 'w') as f:\n",
    "        f.write(\"25 01 10 00\\n\")             # Start year, month, day, hour\n",
    "        f.write(\"1\\n\")                       # Number of starting locations\n",
    "        f.write(\"34.03 -118.33 500\\n\")       # Lat, lon, height (m)\n",
    "        f.write(\"72\\n\")                      # Run duration (hours)\n",
    "        f.write(\"0\\n\")                       # Vertical motion option\n",
    "        f.write(\"10000.0\\n\")                 # Mass release amount\n",
    "        f.write(\"1\\n\")                       # Number of met files\n",
    "        f.write(f\"{met_dir}/\\n\")             # Met file directory\n",
    "        f.write(f\"{met_file}\\n\")             # Met file name\n",
    "        f.write(\"1\\n\")                       # Pollutant type count\n",
    "        f.write(\"PM25\\n\")                    # Pollutant name\n",
    "        f.write(\"1.0\\n\")                     # Emission rate (per hour)\n",
    "        f.write(\"1.0\\n\")                    # Averaging time (hours)\n",
    "        f.write(\"00 00 00 00 00\\n\")          # Release start (yy mm dd hh mm)\n",
    "        f.write(\"1\\n\")                       # Number of grid levels\n",
    "        f.write(\"0.0 0.0\\n\")                 # Grid center (lat, lon)\n",
    "        f.write(\"0.05 0.05\\n\")               # Grid spacing (lat, lon)\n",
    "        f.write(\"40.0 40.0\\n\")               # Grid span (lat, lon)\n",
    "        f.write(\"./\\n\")                      # Output directory\n",
    "        f.write(\"cdump\\n\")                   # Output file name\n",
    "        f.write(\"1\\n\")                       # Number of vertical levels\n",
    "        f.write(\"100\\n\")                     # Height of level (m)\n",
    "        f.write(\"00 00 00 00 00\\n\")          # Sampling start (yy mm dd hh mm)\n",
    "        f.write(\"00 03 00 00 00\\n\")          # Sampling stop (yy mm dd hh mm)\n",
    "        f.write(\"00 01 00\\n\")                # Sampling interval (hh mm ss)\n",
    "        f.write(\"1\\n\")                       # Number of parameters in addition\n",
    "        f.write(\"0.0 0.0 0.0\\n\")             # Particle parameters\n",
    "        f.write(\"0.0 0.0 0.0 0.0 0.0\\n\")     # Deposition parameters\n",
    "        f.write(\"0.0 0.0 0.0\\n\")             # Additional deposition parameters\n",
    "        f.write(\"0.0\\n\")                     # Radioactive decay half-life (days)\n",
    "        f.write(\"0.0\\n\")                     # Pollutant resuspension factor (1/m)\n",
    "    \n",
    "    return control_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e0be7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ascdata_cfg(working_dir, bdy_files_dir):\n",
    "    \"\"\"Create ASCDATA.CFG file for land use data\"\"\"\n",
    "    ascdata_path = os.path.join(working_dir, \"ASCDATA.CFG\")\n",
    "    \n",
    "    with open(ascdata_path, 'w') as f:\n",
    "        f.write(\"-90.0   -180.0  lat/lon of lower left corner\\n\")\n",
    "        f.write(\"1.0     1.0     lat/lon spacing in degrees\\n\")\n",
    "        f.write(\"180     360     lat/lon number of data points\\n\")\n",
    "        f.write(\"2               default land use category\\n\")\n",
    "        f.write(\"0.2             default roughness length (m)\\n\")\n",
    "        f.write(f\"{bdy_files_dir}/  directory of files\\n\")\n",
    "    \n",
    "    return ascdata_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8ddffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_hysplit_conc(exec_path, working_dir, output_dir):\n",
    "    \"\"\"Run the HYSPLIT concentration model\"\"\"\n",
    "    # Change to working directory\n",
    "    original_dir = os.getcwd()\n",
    "    os.chdir(working_dir)\n",
    "    \n",
    "    # Remove previous output files if they exist\n",
    "    if os.path.exists(\"cdump\"):\n",
    "        os.remove(\"cdump\")\n",
    "    if os.path.exists(\"SETUP.CFG\"):\n",
    "        os.remove(\"SETUP.CFG\")\n",
    "    \n",
    "    # Run HYSPLIT\n",
    "    try:\n",
    "        subprocess.run([exec_path], check=True)\n",
    "        print(\"HYSPLIT concentration run completed successfully\")\n",
    "        \n",
    "        # Copy output files to output directory\n",
    "        if os.path.exists(\"cdump\"):\n",
    "            shutil.copy(\"cdump\", os.path.join(output_dir, \"cdump\"))\n",
    "            print(f\"Copied cdump to {output_dir}\")\n",
    "        else:\n",
    "            print(\"Warning: cdump file not created\")\n",
    "        \n",
    "        # Return to original directory\n",
    "        os.chdir(original_dir)\n",
    "        return True\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error running HYSPLIT: {e}\")\n",
    "        # Return to original directory\n",
    "        os.chdir(original_dir)\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992b5dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create configuration files\n",
    "create_ascdata_cfg(working_dir, bdy_files_dir)\n",
    "control_path = create_control_file(output_dir, working_dir, met_file)\n",
    "print(f\"Created CONTROL file at: {control_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73ce211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to executables\n",
    "hycs_std_path = os.path.join(exec_dir, \"hycs_std\")\n",
    "concplot_path = os.path.join(exec_dir, \"concplot\")\n",
    "\n",
    "# Run HYSPLIT concentration model\n",
    "if run_hysplit_conc(hycs_std_path, working_dir, output_dir):\n",
    "    print(f\"Finished! PM2.5 concentration output files are in {output_dir}\")\n",
    "else:\n",
    "    print(\"HYSPLIT run failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca7e2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell to run the con2asc binary from data/conc_output directory\n",
    "!cd data/conc_output && ./con2asc -icdump -ocdump_pm25 cdump_010_01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4695d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare PM2.5 concentration data for ConvLSTM training\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import griddata\n",
    "import pickle\n",
    "\n",
    "# Directory containing cdump files\n",
    "data_dir = \"data/conc_output\"  # Update this to your actual directory\n",
    "output_dir = \"convlstm_data\"   # Directory for processed data\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Function to parse cdump file\n",
    "def parse_cdump(file_path):\n",
    "    \"\"\"Parse HYSPLIT cdump file into a pandas DataFrame\"\"\"\n",
    "    with open(file_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    # Find the data section (after the header)\n",
    "    for i, line in enumerate(lines):\n",
    "        if 'DAY HR' in line:\n",
    "            header_line = i\n",
    "            break\n",
    "    \n",
    "    # Column names from the header line\n",
    "    columns = lines[header_line].strip().split()\n",
    "    \n",
    "    # Parse data lines\n",
    "    data = []\n",
    "    for line in lines[header_line+1:]:\n",
    "        if line.strip():  # Skip empty lines\n",
    "            values = line.strip().split()\n",
    "            if len(values) == len(columns):\n",
    "                data.append(values)\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(data, columns=columns)\n",
    "    \n",
    "    # Convert data types\n",
    "    for col in df.columns:\n",
    "        if col in ['DAY', 'HR']:\n",
    "            df[col] = df[col].astype(int)\n",
    "        elif col in ['LAT', 'LON']:\n",
    "            df[col] = df[col].astype(float)\n",
    "        else:  # PM2500100 or other concentration columns\n",
    "            # Handle scientific notation with 'E' format\n",
    "            df[col] = df[col].apply(lambda x: float(x.replace('E', 'e')))\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Find all cdump files\n",
    "cdump_files = sorted(glob.glob(os.path.join(data_dir, \"cdump_*_*\")))\n",
    "if not cdump_files:\n",
    "    # Try the current directory if data_dir doesn't work\n",
    "    cdump_files = sorted(glob.glob(\"cdump_*_*\"))\n",
    "\n",
    "if not cdump_files:\n",
    "    print(\"No cdump files found. Please check the file path.\")\n",
    "else:\n",
    "    print(f\"Found {len(cdump_files)} cdump files\")\n",
    "    \n",
    "    # Set map boundaries (matches your simulation area)\n",
    "    lat_min, lat_max = 33.6, 34.3\n",
    "    lon_min, lon_max = -118.6, -117.9\n",
    "    \n",
    "    # Create grid for interpolation - 40x40 as requested\n",
    "    grid_resolution = 40\n",
    "    grid_lon = np.linspace(lon_min, lon_max, grid_resolution)\n",
    "    grid_lat = np.linspace(lat_min, lat_max, grid_resolution)\n",
    "    grid_lon_mesh, grid_lat_mesh = np.meshgrid(grid_lon, grid_lat)\n",
    "    \n",
    "    # Container for sequence data\n",
    "    sequence_data = []\n",
    "    sequence_info = []  # To store metadata (day, hour)\n",
    "    \n",
    "    # Process each cdump file\n",
    "    for cdump_file in cdump_files:\n",
    "        # Extract time information from filename\n",
    "        filename = os.path.basename(cdump_file)\n",
    "        day_hour = filename.replace('cdump_', '').split('_')\n",
    "        day = int(day_hour[0])\n",
    "        hour = int(day_hour[1])\n",
    "        \n",
    "        # Parse the cdump file\n",
    "        df = parse_cdump(cdump_file)\n",
    "        \n",
    "        if len(df) > 3:  # Need at least 3 points for interpolation\n",
    "            # Extract coordinates and concentration values\n",
    "            points = df[['LON', 'LAT']].values\n",
    "            values = df.iloc[:, -1].values  # Assuming the last column is concentration\n",
    "            \n",
    "            # Interpolate concentration values onto the standardized grid\n",
    "            # Using linear interpolation instead of cubic\n",
    "            grid_conc = griddata(points, values, (grid_lon_mesh, grid_lat_mesh), \n",
    "                               method='linear', fill_value=0)\n",
    "            \n",
    "            # Save the grid as a numpy array for ConvLSTM training\n",
    "            sequence_data.append(grid_conc)\n",
    "            sequence_info.append({'day': day, 'hour': hour})\n",
    "            \n",
    "            # Also save as visualization for inspection\n",
    "            fig, ax = plt.figure(figsize=(8, 8)), plt.subplot()\n",
    "            im = ax.imshow(grid_conc, origin='lower', cmap='viridis')\n",
    "            plt.colorbar(im, ax=ax, label='PM2.5 Concentration')\n",
    "            plt.title(f'PM2.5 Grid for ConvLSTM - Day {day}, Hour {hour}')\n",
    "            plt.savefig(os.path.join(output_dir, f'pm25_grid_day{day}_hour{hour}.png'), \n",
    "                       dpi=150, bbox_inches='tight')\n",
    "            plt.close(fig)\n",
    "            \n",
    "            print(f\"Processed Day {day}, Hour {hour}\")\n",
    "        else:\n",
    "            print(f\"Not enough data points in {filename} for interpolation\")\n",
    "    \n",
    "    # Convert sequence data to numpy array\n",
    "    sequence_data = np.array(sequence_data)\n",
    "    \n",
    "    # Save as pickle for easy loading in training\n",
    "    with open(os.path.join(output_dir, 'pm25_sequence_data.pkl'), 'wb') as f:\n",
    "        pickle.dump({\n",
    "            'data': sequence_data,\n",
    "            'metadata': sequence_info,\n",
    "            'grid_info': {\n",
    "                'lat_min': lat_min,\n",
    "                'lat_max': lat_max,\n",
    "                'lon_min': lon_min,\n",
    "                'lon_max': lon_max,\n",
    "                'resolution': grid_resolution\n",
    "            }\n",
    "        }, f)\n",
    "    \n",
    "    # Also save as numpy array\n",
    "    np.save(os.path.join(output_dir, 'pm25_sequence_data.npy'), sequence_data)\n",
    "    \n",
    "    print(f\"Saved sequence data with shape: {sequence_data.shape}\")\n",
    "    print(f\"All processing complete. Data saved to {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296aa5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window_of(frames, sample_size, rows, cols, channels):\n",
    "    n_samples = len(frames) - sample_size\n",
    "    samples = np.empty((n_samples, sample_size, rows, cols, channels))\n",
    "    for i in range(n_samples):\n",
    "        samples[i] = np.array([frames[j] for j in range(i, i + sample_size)])\n",
    "        \n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9201f2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "\n",
    "\n",
    "def get_airnow_data(\n",
    "    start_date, end_date, \n",
    "    lon_bottom, lat_bottom, lon_top, lat_top,\n",
    "    airnow_api_key=None\n",
    "):\n",
    "    # get airnow data from the EPA\n",
    "    if os.path.exists('data/airnow.json'):\n",
    "        print(\"data/airnow.json already exists; skipping request...\")\n",
    "    else:\n",
    "        # preprocess a few parameters\n",
    "        date_start = pd.to_datetime(start_date).isoformat()[:13]\n",
    "        date_end = pd.to_datetime(end_date).isoformat()[:13]\n",
    "        bbox = f'{lon_bottom},{lat_bottom},{lon_top},{lat_top}'\n",
    "        URL = \"https://www.airnowapi.org/aq/data\"\n",
    "                \n",
    "        # defining a params dict for the parameters to be sent to the API\n",
    "        PARAMS = {\n",
    "            'startDate':date_start,\n",
    "            'endDate':date_end,\n",
    "            'parameters':'PM25',\n",
    "            'BBOX':bbox,\n",
    "            'dataType':'B',\n",
    "            'format':'application/json',\n",
    "            'verbose':'0',\n",
    "            'monitorType':'2',\n",
    "            'includerawconcentrations':'1',\n",
    "            'API_KEY':airnow_api_key\n",
    "        }\n",
    "        \n",
    "        # sending get request and saving the response as response object\n",
    "        response = requests.get(url = URL, params = PARAMS)\n",
    "    \n",
    "        # extracting data in json format, then download\n",
    "        airnow_data = response.json()\n",
    "        with open('data/airnow.json', 'w') as file:\n",
    "            json.dump(airnow_data, file)\n",
    "            print(\"JSON data saved to data/airnow.json\")\n",
    "        \n",
    "    # open json file and convert to dataframe\n",
    "    with open('data/airnow.json', 'r') as file:\n",
    "        airnow_data = json.load(file)\n",
    "    airnow_df = pd.json_normalize(airnow_data)\n",
    "\n",
    "    # group station data by time\n",
    "    list_df = [group for name, group in airnow_df.groupby('UTC')]\n",
    "    return list_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc7f928",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_ground_sites(df, dim, latMax, latMin, lonMax, lonMin):\n",
    "    latDist, lonDist = abs(latMax - latMin), abs(lonMax - lonMin)\n",
    "    unInter = np.zeros((dim,dim))\n",
    "    dfArr = np.array(df[['Latitude','Longitude','Value']])\n",
    "    for i in range(dfArr.shape[0]):\n",
    "        # Calculate x\n",
    "        x = int(((latMax - dfArr[i,0]) / latDist) * dim)\n",
    "        if x >= dim:\n",
    "            x = dim - 1\n",
    "        if x <= 0:\n",
    "            x = 0\n",
    "        # Calculate y\n",
    "        y = dim - int(((lonMax + abs(dfArr[i,1])) / lonDist) * dim)\n",
    "        if y >= dim:\n",
    "            y = dim - 1\n",
    "        if y <= 0:\n",
    "            y = 0\n",
    "        if dfArr[i,2] < 0:\n",
    "            unInter[x,y] = 0\n",
    "        else:\n",
    "            unInter[x,y] = dfArr[i,2]\n",
    "    return unInter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c697699",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_frame(f, dim):\n",
    "    i = 0\n",
    "    interpolated = []\n",
    "    count = 0\n",
    "    idx = 0\n",
    "    x_list = []\n",
    "    y_list = []\n",
    "    values = []\n",
    "    for x in range(f.shape[0]):\n",
    "        for y in range(f.shape[1]):\n",
    "            if f[x,y] != 0:\n",
    "                x_list.append(x)\n",
    "                y_list.append(y)\n",
    "                values.append(f[x,y])\n",
    "    coords = list(zip(x_list,y_list))\n",
    "    try:\n",
    "        interp = NearestNDInterpolator(coords, values)\n",
    "        X = np.arange(0,dim)\n",
    "        Y = np.arange(0,dim)\n",
    "        X, Y = np.meshgrid(X, Y)\n",
    "        Z = interp(X, Y)\n",
    "    except ValueError:\n",
    "        Z = np.zeros((dim,dim))\n",
    "    interpolated = Z\n",
    "    count += 1\n",
    "    i += 1\n",
    "    interpolated = np.array(interpolated)\n",
    "    return interpolated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9db3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we scale each sample relative to other samples\n",
    "def std_scale(data):\n",
    "    mean = np.mean(data)\n",
    "    stddev = np.std(data)\n",
    "    return (data - mean) / stddev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc6e6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data, process ground sites, interpolate, create samples\n",
    "list_df = get_airnow_data(\n",
    "    start_date, end_date,\n",
    "    lon_bottom, lat_bottom, lon_top, lat_top\n",
    ")\n",
    "list_unInter = [preprocess_ground_sites(df, dim, lat_top, lat_bottom, lon_top, lon_bottom) for df in list_df]\n",
    "list_inter = [interpolate_frame(unInter, dim) for unInter in list_unInter]\n",
    "frames = np.expand_dims(np.array(list_inter), axis=-1)\n",
    "X_airnow = sliding_window_of(frames, frames_per_sample, dim, dim, 1)\n",
    "\n",
    "print(X_airnow.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hysplitenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
